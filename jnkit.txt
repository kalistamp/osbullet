#!/usr/bin/env python3

import requests
from bs4 import BeautifulSoup
import time

def get_all_text(element, level=0):
    """Recursively get all text from an element and its children"""
    texts = []
    
    # Get element's own text
    if element.string and element.string.strip():
        texts.append("  " * level + element.string.strip())
    
    # Get text from children
    for child in element.children:
        if child.name:  # Only process HTML elements
            child_text = get_all_text(child, level + 1)
            texts.extend(child_text)
    
    return texts

def scrape_page(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    }
    
    try:
        # Get the page content
        response = requests.get(url, headers=headers, timeout=30)
        
        # Save raw HTML for debugging
        with open(f'debug_raw_html.txt', 'w', encoding='utf-8') as f:
            f.write(response.text)
            
        # Create BeautifulSoup object
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Get all text content recursively
        main_content = soup.find('main')
        if main_content:
            all_text = get_all_text(main_content)
            return '\n'.join(all_text)
        else:
            return "No main content found"
            
    except Exception as e:
        return f"Error scraping: {str(e)}"

# Main script
print("Starting to scrape website...")
base_url = "https://www.wantstobehired.com"

# Open file to save all content
with open('complete_text_content.txt', 'w', encoding='utf-8') as f:
    # For each page
    for page in range(1, 95):  # 94 pages
        print(f"\nScraping page {page}/94")
        
        # Get URL for current page
        current_url = f"{base_url}?page={page}" if page > 1 else base_url
        print(f"URL: {current_url}")
        
        # Get page content
        content = scrape_page(current_url)
        
        # Write to file with clear separator
        f.write(f"\n\n{'='*50}\nPAGE {page}\n{'='*50}\n\n")
        f.write(content)
        
        # Be nice to the server
        time.sleep(2)

print("\nDone! Check complete_text_content.txt for results")
print("Also check debug_raw_html.txt to see the raw HTML received")
