#!/usr/bin/env python3

import requests
from bs4 import BeautifulSoup
import csv
import time
from datetime import datetime
import sys

class JobScraper:
    def __init__(self):
        self.base_url = "https://www.wantstobehired.com/"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        self.total_pages = 94

    def get_page_content(self, page_num):
        url = f"{self.base_url}?page={page_num}"
        try:
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"\033[91mError fetching page {page_num}: {e}\033[0m")
            return None

    def parse_job_listings(self, html_content):
        if not html_content:
            return []
            
        soup = BeautifulSoup(html_content, 'html.parser')
        jobs = []
        
        # Find all job cards
        job_cards = soup.find_all('div', class_='border rounded-md m-2 px-5 py-5 bg-white')
        print(f"\nFound {len(job_cards)} job cards on this page")
        
        for card in job_cards:
            # Skip promoted content
            if 'Promoted' in card.text:
                continue
                
            job = {}
            
            # Extract fields
            for field in card.find_all('span', class_='font-semibold'):
                field_name = field.text.strip(':').lower().replace(' ', '_')
                field_value = field.find_next(text=True)
                if field_value:
                    job[field_name] = field_value.strip()
            
            # Get HN link if available
            hn_link = card.find('a', href=lambda x: x and 'news.ycombinator.com' in x)
            if hn_link:
                job['hn_link'] = hn_link['href']
            
            if job:
                jobs.append(job)
                
        return jobs

    def save_to_csv(self, all_jobs):
        if not all_jobs:
            print("\033[91mNo jobs found to save!\033[0m")
            return
            
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"wantstobehired_jobs_{timestamp}.csv"
        
        # Get all unique fields from all jobs
        fieldnames = set()
        for job in all_jobs:
            fieldnames.update(job.keys())
        fieldnames = sorted(list(fieldnames))
        
        try:
            with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(all_jobs)
            print(f"\n\033[92mData saved to {filename}\033[0m")
            print(f"\033[92mTotal jobs collected: {len(all_jobs)}\033[0m")
        except Exception as e:
            print(f"\033[91mError saving data: {e}\033[0m")

    def run(self):
        print("\033[94mStarting job scraping...\033[0m")
        all_jobs = []
        
        for page in range(1, self.total_pages + 1):
            print(f"\nProcessing page {page}/{self.total_pages}")
            content = self.get_page_content(page)
            if content:
                jobs = self.parse_job_listings(content)
                if jobs:
                    all_jobs.extend(jobs)
                    print(f"Added {len(jobs)} jobs from page {page}")
                time.sleep(2)  # Be nice to the server
        
        print("\n\033[94mScraping completed!\033[0m")
        self.save_to_csv(all_jobs)

if __name__ == "__main__":
    print("\033[95m=== WantsToBeHired Job Scraper ===\033[0m")
    scraper = JobScraper()
    scraper.run()
