#!/usr/bin/env python3
# pip3 install selenium webdriver_manager

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import csv
import time
from datetime import datetime
import sys

class JobScraper:
    def __init__(self):
        self.base_url = "https://www.wantstobehired.com/"
        self.setup_driver()
        self.total_pages = 94

    def setup_driver(self):
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        self.driver = webdriver.Chrome(options=chrome_options)

    def get_page_content(self, page_num):
        url = f"{self.base_url}?page={page_num}"
        try:
            self.driver.get(url)
            # Wait for job listings to load
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.CLASS_NAME, "border.rounded-md.m-2.px-5.py-5.bg-white"))
            )
            print(f"\nFound content on page {page_num}")
            return True
        except Exception as e:
            print(f"\n\033[91mError on page {page_num}: {e}\033[0m")
            return False

    def parse_job_listings(self):
        jobs = []
        try:
            # Find all job cards
            job_cards = self.driver.find_elements(By.CSS_SELECTOR, 'div.border.rounded-md.m-2.px-5.py-5.bg-white')
            print(f"Found {len(job_cards)} job cards")

            for card in job_cards:
                # Skip promoted content
                if 'Promoted' in card.text:
                    continue

                job = {}
                try:
                    # Extract date
                    date_elem = card.find_element(By.CSS_SELECTOR, 'p:has(span.font-semibold:contains("Date"))')
                    job['date'] = date_elem.text.replace('Date:', '').strip()

                    # Extract email
                    try:
                        email_elem = card.find_element(By.CSS_SELECTOR, 'p:has(span.font-semibold:contains("Email"))')
                        job['email'] = email_elem.text.replace('Email:', '').strip()
                    except:
                        job['email'] = ''

                    # Extract location
                    try:
                        location_elem = card.find_element(By.CSS_SELECTOR, 'p:has(span.font-semibold:contains("Location"))')
                        job['location'] = location_elem.text.replace('Location:', '').strip()
                    except:
                        job['location'] = ''

                    # Extract technologies
                    try:
                        tech_elem = card.find_element(By.CSS_SELECTOR, 'p:has(span.font-semibold:contains("Technologies"))')
                        job['technologies'] = tech_elem.text.replace('Technologies:', '').strip()
                    except:
                        job['technologies'] = ''

                    # Extract HN link
                    try:
                        hn_link = card.find_element(By.CSS_SELECTOR, 'a[href*="news.ycombinator.com"]')
                        job['hn_link'] = hn_link.get_attribute('href')
                    except:
                        job['hn_link'] = ''

                    jobs.append(job)
                except Exception as e:
                    print(f"\033[93mError parsing job card: {e}\033[0m")
                    continue

        except Exception as e:
            print(f"\033[91mError parsing page: {e}\033[0m")

        return jobs

    def save_to_csv(self, all_jobs):
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"wantstobehired_jobs_{timestamp}.csv"
        
        if all_jobs:
            fieldnames = ['date', 'email', 'location', 'technologies', 'hn_link']
            
            with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(all_jobs)
            
            print(f"\n\033[92mData saved to {filename}\033[0m")
            print(f"\033[92mTotal jobs collected: {len(all_jobs)}\033[0m")
        else:
            print("\033[91mNo data to save!\033[0m")

    def run(self):
        all_jobs = []
        
        print("\033[94mStarting job scraping...\033[0m")
        for page in range(1, self.total_pages + 1):
            sys.stdout.write(f"\rProcessing page {page}/{self.total_pages}")
            sys.stdout.flush()
            
            if self.get_page_content(page):
                jobs = self.parse_job_listings()
                all_jobs.extend(jobs)
                print(f"Collected {len(jobs)} jobs from page {page}")
                
                # Be nice to the server
                time.sleep(2)
        
        print("\n\033[94mScraping completed!\033[0m")
        self.save_to_csv(all_jobs)
        self.driver.quit()

if __name__ == "__main__":
    print("\033[95m=== WantsToBeHired Job Scraper ===\033[0m")
    scraper = JobScraper()
    scraper.run()
