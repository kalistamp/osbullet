#!/usr/bin/env python3

import requests
from bs4 import BeautifulSoup
import time

def extract_content(soup):
    """Extract all visible content from job listings"""
    all_content = []
    
    # Find all job listing divs (these contain the actual content we want)
    listings = soup.find_all('div', class_='border rounded-md m-2 px-5 py-5 bg-white')
    
    for listing in listings:
        # Skip any promotional content
        if 'Promoted' in listing.text:
            continue
            
        content = []
        # Get all paragraphs from the listing
        for p in listing.find_all('p'):
            text = p.get_text(strip=True)
            if text:
                content.append(text)
                
        # Get any links (like HN links)
        for a in listing.find_all('a'):
            if 'news.ycombinator.com' in a.get('href', ''):
                content.append(f"HN Link: {a['href']}")
                
        if content:
            all_content.append('\n'.join(content))
            all_content.append('-' * 50)  # Add separator between listings
            
    return '\n'.join(all_content)

def scrape_page(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    try:
        print(f"Fetching: {url}")
        response = requests.get(url, headers=headers, timeout=30)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # First save the raw HTML for debugging
        with open('last_page_raw.html', 'w', encoding='utf-8') as f:
            f.write(response.text)
            
        return extract_content(soup)
        
    except Exception as e:
        print(f"Error scraping: {str(e)}")
        return ""

# Main script
print("Starting to scrape job listings...")
base_url = "https://www.wantstobehired.com"

with open('job_listings.txt', 'w', encoding='utf-8') as f:
    for page in range(1, 95):  # 94 pages total
        print(f"\nProcessing page {page}/94")
        
        current_url = f"{base_url}?page={page}" if page > 1 else base_url
        content = scrape_page(current_url)
        
        if content.strip():
            f.write(f"\n\n{'='*20} PAGE {page} {'='*20}\n\n")
            f.write(content)
            print(f"Found content on page {page}")
        else:
            print(f"No content found on page {page}")
        
        # Also save the current page content to a separate file
        with open(f'page_{page}_content.txt', 'w', encoding='utf-8') as page_file:
            page_file.write(content)
        
        time.sleep(2)  # Be nice to the server

print("\nDone! Check these files:")
print("1. job_listings.txt - Contains all listings")
print("2. page_X_content.txt - Individual page contents")
print("3. last_page_raw.html - Raw HTML of last page fetched")
